{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from rightAlignedConv1d import rac1d\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"yuge.txt\", \"rt\") as f:\n",
    "with open(\"out\", \"rt\") as f:\n",
    "    # tdata = f.read().replace(\"\\n\",\"\").replace(\"\\t\", \"\")\n",
    "    tdata = f.read().replace(\"\\t\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(set(tdata))\n",
    "token_to_idx = dict(zip(unique_tokens, range(len(unique_tokens))))\n",
    "idx_to_token = dict(zip(range(len(unique_tokens)), unique_tokens))\n",
    "idx_data = list(map(token_to_idx.get, tdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_range(lenx, seq_len):\n",
    "    start_i = np.random.randint(0, lenx-seq_len-1) # allow for one predicted and input token at the end\n",
    "    return start_i, start_i + seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sample(data, seq_len):\n",
    "    start_bound, end_bound = get_random_range(len(data), seq_len)\n",
    "    return torch.tensor([data[start_bound: end_bound]], requires_grad=False), torch.tensor([data[start_bound+1: end_bound+1]], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_batch(data, seq_len, num_sample=100):\n",
    "    xs, ys = [], []\n",
    "    for _ in range(num_sample):\n",
    "        gx, gy = generate_random_sample(data, seq_len)\n",
    "        xs.append(gx)\n",
    "        ys.append(gy)\n",
    "    return torch.vstack(xs), torch.vstack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_str(idxs):\n",
    "    return \"\".join([idx_to_token[idx] for idx in idxs]).replace(\"\\n\",\" <ENTER> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tens(text):\n",
    "    return torch.tensor([list(map(token_to_idx.get, text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one(text, model, sampler = lambda x: x.argmax()):\n",
    "    new_char = sampler(model(text_to_tens(text).cuda())[0,:,-1]).item()\n",
    "    return text + idx_to_token[new_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elaborate(text, model, num=10, sampler = lambda x: x.argmax()):\n",
    "    ctex = text\n",
    "    for _ in tqdm(range(num)):\n",
    "        ctex = gen_one(ctex, model, sampler)\n",
    "    return ctex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skippableMax(x: torch.Tensor) -> torch.Tensor:\n",
    "    x_orig = x\n",
    "    x = torch.cummax(x, 2).values\n",
    "    return x_orig + x\n",
    "    # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelBlock(nn.Module):\n",
    "    def __init__(self, nchannel, kernelsize) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = rac1d(nchannel, nchannel, kernelsize, padding=\"same\")\n",
    "        self.conv2 = rac1d(nchannel, nchannel, kernelsize, padding=\"same\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_orig = x\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.gelu(x)\n",
    "        # x = torch.relu(x)\n",
    "        # x = skippableMax(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.gelu(x)\n",
    "        # x = torch.relu(x)\n",
    "        # x = skippableMax(x)\n",
    "\n",
    "        output = x + x_orig # should already be relu-ed\n",
    "        # output = torch.renorm(output, 2, -1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TModel(nn.Module):\n",
    "    def __init__(self, char_num=67, kern_size=32, chan_count=140) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(char_num, chan_count)\n",
    "        self.drop = nn.Dropout1d(p=0.1)\n",
    "        self.block1 = modelBlock(chan_count, kern_size)\n",
    "\n",
    "        self.block2 = modelBlock(chan_count, kern_size)\n",
    "        self.block2_1 = modelBlock(chan_count, kern_size)\n",
    "        self.block2_2 = modelBlock(chan_count, kern_size)\n",
    "\n",
    "        self.block3 = modelBlock(chan_count, kern_size*2)\n",
    "        self.output_conv = rac1d(chan_count, char_num, kern_size, padding=\"same\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, debug=False) -> torch.Tensor:\n",
    "        # x = self.embedding(x).permute(1, 2, 0)\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        # if debug: print(f\"{x=}\")\n",
    "        # if debug: print(f\"{x.shape=}\")\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # x = self.block1(x)\n",
    "\n",
    "        # x = self.block2(x)\n",
    "        # x = self.block2_1(x)\n",
    "        # x = self.block2_2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        \n",
    "        # x = skippableMax(x)\n",
    "        # x = skippableMax(x)\n",
    "        \n",
    "        x = self.output_conv(x)\n",
    "\n",
    "        # if debug: print(f\"{x=}\")\n",
    "        # if debug: print(f\"{x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQ_LEN = 96\n",
    "# SEQ_LEN = 256\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "# BATCH_SIZE = 32\n",
    "CHAR_COUNT = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TModel(CHAR_COUNT)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(generate_random_batch(idx_data, SEQ_LEN, BATCH_SIZE)[0].cuda(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), capturable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "print(f\"{model.training=}\")\n",
    "print(f\"{model=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_input = torch.randint(0, CHAR_COUNT, (BATCH_SIZE, SEQ_LEN), device='cuda').long()\n",
    "# static_target = torch.randint(0, CHAR_COUNT, (BATCH_SIZE, SEQ_LEN), device='cuda').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = torch.cuda.Stream()\n",
    "# s.wait_stream(torch.cuda.current_stream())\n",
    "# with torch.cuda.stream(s):\n",
    "#     for i in range(3):\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         y_pred = model(static_input)\n",
    "#         loss = criterion(y_pred, static_target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# torch.cuda.current_stream().wait_stream(s)\n",
    "\n",
    "# # capture\n",
    "# g = torch.cuda.CUDAGraph()\n",
    "# # Sets grads to None before capture, so backward() will create\n",
    "# # .grad attributes with allocations from the graph's private pool\n",
    "# optimizer.zero_grad(set_to_none=True)\n",
    "# with torch.cuda.graph(g):\n",
    "#     static_y_pred = model(static_input)\n",
    "#     static_loss = criterion(static_y_pred, static_target)\n",
    "#     static_loss.backward()\n",
    "#     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(f\"{model.training=}\")\n",
    "print(f\"{model=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(100)):\n",
    "#     data, target = generate_random_batch(idx_data, SEQ_LEN, BATCH_SIZE)\n",
    "#     static_input.copy_(data)\n",
    "#     static_target.copy_(target)\n",
    "#     g.replay()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f\"ep {epoch}\")\n",
    "\n",
    "#         outputs = model(data.cuda())\n",
    "\n",
    "#         print(f\"acc={torch.mean((outputs.argmax(1) == target.cuda()).float())}\")\n",
    "#         print(f\"LAST SAMPLE EXP: {idxs_to_str(target[-1].tolist())}\")\n",
    "#         print(f\"LAST SAMPLE MES: {idxs_to_str(outputs[-1].argmax(0).tolist())}\")\n",
    "\n",
    "#         print(f\"{loss.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(100)):\n",
    "#     data, target = generate_random_batch(idx_data, SEQ_LEN, BATCH_SIZE)\n",
    "#     data = data.cuda()\n",
    "#     target = target.cuda()\n",
    "    \n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     y_pred = model(data)\n",
    "#     loss = criterion(y_pred, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRTracker import LinearLRTracker\n",
    "llrt = LinearLRTracker(5e-4, .0015, 10, 100)\n",
    "# llrt = LinearLRTracker(5e-4, .003, 100, 100)\n",
    "\n",
    "try:\n",
    "    # Creates once at the beginning of training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in tqdm(range(llrt.num_steps)):\n",
    "\n",
    "        newLR = llrt.lr_step()\n",
    "        # print(f\"{newLR=}\")\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = newLR\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        data, target = generate_random_batch(idx_data, SEQ_LEN, BATCH_SIZE)\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # Casts operations to mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_pred = scripted_model(data)\n",
    "            loss = criterion(y_pred, target)\n",
    "\n",
    "        # Scales the loss, and calls backward()\n",
    "        # to create scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales gradients and calls\n",
    "        # or skips optimizer.step()\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "        llrt.record_loss(loss.item())\n",
    "except StopIteration as ex:\n",
    "    print(f\"Stopped because: {ex}\")\n",
    "\n",
    "EXLR = llrt.findLR()\n",
    "print(f\"{EXLR=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(llrt.steps[:len(llrt.losses)], llrt.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TModel(CHAR_COUNT)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), capturable = True, lr=EXLR)\n",
    "scripted_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 14712/50000 [04:10<09:41, 60.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 14700\n",
      "acc=0.837646484375\n",
      "LAST SAMPLE EXP:    super.onCreate(savedInstanceState); <ENTER>         // Sets the content view to the xml file. <ENTER>         setContentView(R.layout.main_ac\n",
      "LAST SAMPLE MES:     uper(onCreate()acedInstanceState); <ENTER>         // Set  the content view to the rme tile  <ENTER>          etContentValw(r.layout.main_ac\n",
      "loss.item()=0.6408604979515076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 14808/50000 [04:12<10:33, 55.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 14800\n",
      "acc=0.8421630859375\n",
      "LAST SAMPLE EXP:  <ENTER>                  * Hook method that decides whether to terminate the <ENTER>                  * phaser or not. <ENTER>                  */ <ENTER>     \n",
      "LAST SAMPLE MES:       *             cook method that rerises whenher th shr inato aoe                  * Ehrled ou doo . *       *      e / <ENTER>     \n",
      "loss.item()=0.6221038103103638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 14912/50000 [04:14<09:34, 61.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 14900\n",
      "acc=0.8548583984375\n",
      "LAST SAMPLE EXP: g>) String::isEmpty).negate()) <ENTER>                 // Collect the results into a string. <ENTER>                 .collect(toList()); <ENTER>        \n",
      "LAST SAMPLE MES: g  ;{tring :ps.npty).negate()) <ENTER>                 // Collect the results into l string. <ENTER>                 .collect(toList()); <ENTER>        \n",
      "loss.item()=0.5656384825706482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15010/50000 [04:15<09:27, 61.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15000\n",
      "acc=0.869140625\n",
      "LAST SAMPLE EXP:  0); <ENTER>     } <ENTER>     /** <ENTER>      * FAB animator that displays the FAB. <ENTER>      * @param fab The FAB to be displayed <ENTER>      */ <ENTER>     public stati\n",
      "LAST SAMPLE MES:  R0; <ENTER>       <ENTER>     /** <ENTER>      * ToB fnvmator trat aisplays mhe FAB. <ENTER>      */@param fo  The FAB to be desplayed <ENTER>      */ <ENTER>     public stati\n",
      "loss.item()=0.5008204579353333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15107/50000 [04:17<09:31, 61.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15100\n",
      "acc=0.843994140625\n",
      "LAST SAMPLE EXP: eate a Folder containing all <ENTER>             // the subfolders and documents in this folder. <ENTER>             .collect(FolderCollector.to\n",
      "LAST SAMPLE MES: etde a rurder fontaining tll             /* che suplolders and dotements in thes folder. <ENTER>             .colsect(Folder(ollector.to\n",
      "loss.item()=0.6279701590538025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15212/50000 [04:18<09:20, 62.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15200\n",
      "acc=0.8448486328125\n",
      "LAST SAMPLE EXP: **  <ENTER>      * Tracks the mPrecedence of the expression. <ENTER>      */ <ENTER>     private int mAccumulatedPrecedence; <ENTER>     /**  <ENTER>      * The mPreced\n",
      "LAST SAMPLE MES:  <ENTER> * <ENTER>  <ENTER>      * Chant yahe rIlivedence of che txpression       */ <ENTER>     puivate snt mActutulatedPrecedence; <ENTER>     /** <ENTER>  <ENTER>      * the npaeced\n",
      "loss.item()=0.6086302995681763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15310/50000 [04:20<09:19, 62.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15300\n",
      "acc=0.860107421875\n",
      "LAST SAMPLE EXP: tor is also passed a filled in resultList. <ENTER>      */ <ENTER>     public SearchResults(String word, <ENTER>                          String title, <ENTER> \n",
      "LAST SAMPLE MES: tiry=n t lo pars d arbolted in tesult ist. <ENTER>      *  <ENTER>     public StarchResult((String word, <ENTER>                          String title, <ENTER> \n",
      "loss.item()=0.532831072807312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15408/50000 [04:22<09:20, 61.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15400\n",
      "acc=0.82763671875\n",
      "LAST SAMPLE EXP: urce> resources, <ENTER>          AbstractMap<Resource, LeaseState> resourceMap) { <ENTER>         // Call the superclass constructor. <ENTER>         s\n",
      "LAST SAMPLE MES: nnce  resuurces  <ENTER>           bspractMap<Resource, LeaseState> resourceMap) { <ENTER>         // Call the superclass constructor. <ENTER>         s\n",
      "loss.item()=0.6420284509658813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15511/50000 [04:23<09:46, 58.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15500\n",
      "acc=0.8172607421875\n",
      "LAST SAMPLE EXP: ft and mRight children.  The meaning of this <ENTER>  * node is mLeft / mRight.  It plays the role of a \"Composite\" in the <ENTER>  * Composite \n",
      "LAST SAMPLE MES: rt tnd reemht =oild en   The saaning of this <ENTER>  * node is mLeft c mRight.  It plays the role of a \"Componite\" in the <ENTER>  * Composite \n",
      "loss.item()=0.7152917981147766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15607/50000 [04:25<09:33, 59.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15600\n",
      "acc=0.8250732421875\n",
      "LAST SAMPLE EXP: e a stream that traverses all nodes in the expression tree and <ENTER>         // accepts the evalVisitor to evaluate each type of node.\n",
      "LAST SAMPLE MES:   o stream ohot whaverses arl tode  tf the @xpression tree and         // actepts the pvarVasitor to evaluase fxch type of node.\n",
      "loss.item()=0.6937602162361145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 15712/50000 [04:27<09:30, 60.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15700\n",
      "acc=0.8477783203125\n",
      "LAST SAMPLE EXP: tEquals; <ENTER> import static org.junit.Assert.assertNotEquals; <ENTER> /** <ENTER>  * Tests the features of the PhraseMatchTask.  Thanks to Sanjeev Ku\n",
      "LAST SAMPLE MES: e juals( <ENTER> import jtatic jre.junit.As.ert.assertEod(quals( <ENTER> i** <ENTER>  * Tests toe crature  if ahe drrase,atchTask.  Thanks.th sarjeev Ku\n",
      "loss.item()=0.5644977688789368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 15808/50000 [04:28<09:30, 59.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15800\n",
      "acc=0.853271484375\n",
      "LAST SAMPLE EXP: eger. <ENTER>                     BigInteger numerator = <ENTER>                         new BigInteger(150000, random); <ENTER>                     // \n",
      "LAST SAMPLE MES:  rer. <ENTER>      *               igFnteger f1merator,=                         new wigInteger(150000, nandod); <ENTER>                     // \n",
      "loss.item()=0.5564645528793335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 15912/50000 [04:30<09:29, 59.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 15900\n",
      "acc=0.8358154296875\n",
      "LAST SAMPLE EXP:  { <ENTER>     /**  <ENTER>      * Holds the expression tree that is the target of the commands. <ENTER>      */ <ENTER>     private TreeOps mTreeOps; <ENTER>  <ENTER>     /** \n",
      "LAST SAMPLE MES:  i <ENTER>     /** <ENTER>  <ENTER>      * Teod  the hxpression tree that is the target of the <ENTER> commands. <ENTER>      */ <ENTER>     private TreeOps TPreeOps; <ENTER>      /** \n",
      "loss.item()=0.6410050988197327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16008/50000 [04:32<09:54, 57.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16000\n",
      "acc=0.842041015625\n",
      "LAST SAMPLE EXP: ing() <ENTER>                                                      + \"\\n\")); <ENTER>                               display(sb.toString()); <ENTER>      \n",
      "LAST SAMPLE MES: eng )                                                        \" \"\");; <ENTER>                         /      isplay(wbetoString()); <ENTER>      \n",
      "loss.item()=0.6169678568840027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16110/50000 [04:33<09:29, 59.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16100\n",
      "acc=0.860595703125\n",
      "LAST SAMPLE EXP:   */ <ENTER>     @SuppressLint(\"InflateParams\") <ENTER>     public int addURLs(View view) { <ENTER>     // Create the new list from R.layout.list_item. <ENTER> \n",
      "LAST SAMPLE MES:      <ENTER>     pOeppressWink(\"Nntlateharams\") <ENTER>      ublic int gpdIRLs()iew view) { <ENTER>      / Sreate the few uist ooom ieldtout UistsinAm. <ENTER> \n",
      "loss.item()=0.5489756464958191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16211/50000 [04:35<09:11, 61.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16200\n",
      "acc=0.834228515625\n",
      "LAST SAMPLE EXP: Executors.newFixedThreadPool(threadPoolSize)); <ENTER>         // Call up to superclass to start the processing. <ENTER>         super.initiateS\n",
      "LAST SAMPLE MES: )xpcutorC.oewFuxedThreadPool()hiead.oolTtze(;; <ENTER>         // Prllsts th shmer.cass th start the provessIng. <ENTER>         super.inNtiateS\n",
      "loss.item()=0.663445770740509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16309/50000 [04:37<09:03, 62.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16300\n",
      "acc=0.87060546875\n",
      "LAST SAMPLE EXP: eStrategy\" in the Strategy pattern <ENTER>  *        that defines the pre-order iteration algorithm.  <ENTER>  */ <ENTER> public class LevelOrderIterato\n",
      "LAST SAMPLE MES: ertretegy  in the Strategy pattern    @       hat defunes the pre-order iteration algorithm. <ENTER>  <ENTER>  *  <ENTER> public class SeaelOrderIterato\n",
      "loss.item()=0.5057509541511536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16407/50000 [04:38<09:01, 62.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16400\n",
      "acc=0.8411865234375\n",
      "LAST SAMPLE EXP: ay of input strings. <ENTER>             .forEach(listOfStrings -> { <ENTER>                     // The results are stored in a list of input st\n",
      "LAST SAMPLE MES: et ff tmput strings  <ENTER>      *       corEach(tost n(tring:(-- { <ENTER>                     // Ihe tesults ate toored wn t rist of tmput se\n",
      "loss.item()=0.5975852012634277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16506/50000 [04:40<09:53, 56.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16500\n",
      "acc=0.8560791015625\n",
      "LAST SAMPLE EXP: (), <ENTER>                         // which transforms each URL to a completable <ENTER>                         // future to an image (i.e., a\n",
      "LAST SAMPLE MES: tS                           / Raech caansfrrme tnch tRL uo \" lompletable <ENTER>                         // future to an image (ite., f\n",
      "loss.item()=0.5411132574081421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16608/50000 [04:42<09:30, 58.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16600\n",
      "acc=0.8212890625\n",
      "LAST SAMPLE EXP:  <ENTER>                 inSampleSize *= 2; <ENTER>             } <ENTER>         } <ENTER>  <ENTER>         return inSampleSize; <ENTER>     } <ENTER> } <ENTER> // import javax.imageio.ImageI\n",
      "LAST SAMPLE MES:  <ENTER>         /        ftecelettze;=/ <ENTER> n; <ENTER>               <ENTER>         } <ENTER>          return tn-empleStze; <ENTER>     } <ENTER> } <ENTER> /* Umport java..imatero.TmageI\n",
      "loss.item()=0.7009772658348083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16714/50000 [04:44<09:15, 59.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16700\n",
      "acc=0.8380126953125\n",
      "LAST SAMPLE EXP: terNameLengths); <ENTER>     } <ENTER>     /** <ENTER>      * Run an example using the collect(groupingBy()) and the <ENTER>      * collect(summingInt()) termin\n",
      "LAST SAMPLE MES: hiryame(ingeh( ; <ENTER>       <ENTER>     /** <ENTER>      * Cen an example using the collect(teoupungBy(). and the <ENTER>      * collect(nummingInt()) <ENTER> thrmin\n",
      "loss.item()=0.6294669508934021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 16812/50000 [04:45<08:57, 61.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16800\n",
      "acc=0.832275390625\n",
      "LAST SAMPLE EXP: amlet,laertes,Ophelia\") <ENTER>             // Remove any strings that don't start with 'h' or 'H'. <ENTER>             .filter(s -> toLowerCase\n",
      "LAST SAMPLE MES: asFet  <ENTER> esns:st <ENTER> ntelect);            // Resove any strings that don't start with 'h' or 'H'. <ENTER>             .filter(s -> toLowerCase\n",
      "loss.item()=0.6439902782440186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 16907/50000 [04:47<09:25, 58.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 16900\n",
      "acc=0.8380126953125\n",
      "LAST SAMPLE EXP:  the results map. <ENTER>         mResultsMap.put(testName, <ENTER>                 mExecutionTime); <ENTER>     } <ENTER>     /** <ENTER>      * @return A string conta\n",
      "LAST SAMPLE MES: rthe desults.iaps <ENTER>         mRetultsMap.put(testName, <ENTER>                  ExecutionTime); <ENTER>     } <ENTER>     /** <ENTER>      * @return A string conta\n",
      "loss.item()=0.6469748020172119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17009/50000 [04:48<10:08, 54.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 17000\n",
      "acc=0.8643798828125\n",
      "LAST SAMPLE EXP:          <ENTER>         // Update \"this\" PhraseMatchTask to handle the \"right hand\" <ENTER>         // portion of the input. <ENTER>         mInput = m\n",
      "LAST SAMPLE MES:                   / Cslate t <ENTER> his\" ahraseMatchTask to sandle the \"right hand\" <ENTER>          / prrtion of the input. <ENTER>         mInput = m\n",
      "loss.item()=0.5335625410079956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17111/50000 [04:50<09:23, 58.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 17100\n",
      "acc=0.855224609375\n",
      "LAST SAMPLE EXP:            else <ENTER>                     // Negate operation. <ENTER>                     latestSymbol = new Negate(); <ENTER>                 break;\n",
      "LAST SAMPLE MES:             xse                      / Oomate tferations <ENTER>                      otept ymbol = new Sadete(); <ENTER>                 ireak;\n",
      "loss.item()=0.5645860433578491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17212/50000 [04:52<09:13, 59.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 17200\n",
      "acc=0.84228515625\n",
      "LAST SAMPLE EXP: String mPathUri = \"index.html\"; <ENTER>     /** <ENTER>      * Controls whether debugging output will be generated (defaults <ENTER>      * to false). <ENTER>  \n",
      "LAST SAMPLE MES:  tring tIrrh)Ni = pinserEhrai:) <ENTER>     /** <ENTER>      * Controls whether debuiging output will be generated (default  <ENTER>      * to false)  <ENTER>  \n",
      "loss.item()=0.6201860308647156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 17307/50000 [04:54<08:52, 61.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 17300\n",
      "acc=0.8154296875\n",
      "LAST SAMPLE EXP: makeStrategy  <ENTER>                                  (List<Resource> resources,  <ENTER>                                   LeasePool.SyncStrat\n",
      "LAST SAMPLE MES:  ete taitegy =                                  Sist<Sesource> resources, <ENTER>  <ENTER>                                   LiasePool.SyncStrat\n",
      "loss.item()=0.7232285737991333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 17411/50000 [04:55<09:10, 59.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 17400\n",
      "acc=0.8565673828125\n",
      "LAST SAMPLE EXP:     /** <ENTER>      * Constructor creates a new interpreter that handles user input <ENTER>      * supplied via the given {@code format} using \n",
      "LAST SAMPLE MES:      ** <ENTER>      * Ronstructor ioeates alLew Lnterpreter that handles ased input      * sumplied vie the given {@code nirmat} csing \n",
      "loss.item()=0.5548886060714722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 17488/50000 [04:57<09:32, 56.77it/s]"
     ]
    }
   ],
   "source": [
    "# Creates once at the beginning of training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(50000)):\n",
    "   # optimizer.zero_grad()\n",
    "   optimizer.zero_grad(set_to_none=True)\n",
    "   data, target = generate_random_batch(idx_data, SEQ_LEN, BATCH_SIZE)\n",
    "   data = data.cuda()\n",
    "   target = target.cuda()\n",
    "   \n",
    "   # Casts operations to mixed precision\n",
    "   with torch.cuda.amp.autocast():\n",
    "      y_pred = scripted_model(data)\n",
    "      loss = criterion(y_pred, target)\n",
    "\n",
    "   # Scales the loss, and calls backward()\n",
    "   # to create scaled gradients\n",
    "   scaler.scale(loss).backward()\n",
    "\n",
    "   # Unscales gradients and calls\n",
    "   # or skips optimizer.step()\n",
    "   scaler.step(optimizer)\n",
    "\n",
    "   # Updates the scale for next iteration\n",
    "   scaler.update()\n",
    "\n",
    "   if epoch % 100 == 0:\n",
    "      with torch.no_grad():\n",
    "         print(f\"ep {epoch}\")\n",
    "         outputs = scripted_model(data.cuda())\n",
    "         print(f\"acc={torch.mean((outputs.argmax(1) == target.cuda()).float())}\")\n",
    "         print(f\"LAST SAMPLE EXP: {idxs_to_str(target[-1].tolist())}\")\n",
    "         print(f\"LAST SAMPLE MES: {idxs_to_str(outputs[-1].argmax(0).tolist())}\")\n",
    "         print(f\"{loss.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elaborate(\"for(\", model, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c8158d981000c5a34bf32431467cf5cb93ec2272b8d464bd409096bcc51e96a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
